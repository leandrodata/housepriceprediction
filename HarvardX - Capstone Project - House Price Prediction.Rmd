---
title: "Harvard Capstone Project - House Price Prediction"
author: "Leandro Rodrigues Carvalho"
date: "07/03/2021"
output:
  pdf_document: default
  html_document: default
---

# Introduction

This is an R Markdown project aiming to develop a model to predict house selling prices, developed as part of Harvard University online Professional Certificate course on Data Science, lead by Professor Rafael A. Irizarry.

The *goal* of this project is to predict homes final selling prices using a data set from Kaggle's educational competition "House Prices - Advanced Regression Techniques". The data set includes 79 explanatory variables of residential homes in Ames, Iowa, USA.

The *key steps* that were performed included Exploratory Data Analysis (EDA), data visualization, data wrangling and training the machine learning algorithm using the inputs in one subset to predict selling prices in the test set.

# Acknowledgments

(<https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview>)

*"The Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset"*

The original study can be found on the link below, where Professor Dean De Cock, from Truman State University, explain the process of accessing the data and preparing it for further use.

<http://jse.amstat.org/v19n3/decock.pdf>

# Data & Key Questions

Every home buyer asks some of the basic questions in relation to a property to asses if the sales price is fair. When was it built? How big is the lot? How many square feet of living space is in the dwelling? Is the basement finished? How many bathrooms are there? The variables contained in this data set contain the information that a typical home buyer would want to know when buying a house.

As mentioned initially, the data set describes the sale of individual residential property in Ames, Iowa, from 2006 to 2010, and contains 2930 observations and a large number of explanatory variables (23 nominal, 23 ordinal, 14 discrete, and 20 continuous) involved in assessing home values.

## Missing packages are automatically installed with external link

This code creates a training set and a validation, or test, set (final hold-out test set)

```{r message=FALSE, warning=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(gbm)) install.packages("gbm", repos = "http://cran.us.r-project.org")
if(!require(rpart)) install.packages("rpart", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")

```

## Loading packages

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(caret)
library(data.table)
library(randomForest)
library(gbm)
library(rpart)
library(knitr)
```

## Retrieving train and test sets

```{r message=FALSE, warning=FALSE}
df <- read.csv("/Users/leandrocarvalho/Documents/Education/HarvardX - Data Science/HarvardX9 - Capstone Project/HarvardX9 - House Price Prediction/house-prices-advanced-regression-techniques/train.csv")

row.names(df) <- df$Id
df <- df[,-1]
df[is.na(df)] <- 0
for(i in colnames(df[,sapply(df, is.character)])){
  df[,i] <- as.factor(df[,i])
}

### Test values
test.n <- sample(1:nrow(df), nrow(df)/3, replace = F)

### Test dataset
test <- df[test.n,]

### Train dataset
train <- df[-test.n,]

rm(test.n, df)

### Combined Data Frame
combined.df <- rbind(test, train)
```

# Method and Analyses

### Exploring multiple statistical methods

We used several packages to explore multiple statistical methods, computing the RMSE for the trained models: Linear Regression, Regression Tree, Random Forest and Gradient Boosting Machine.

### Understanding of the data set

The combined Data Set is made of 80 variables and 1460 observations, including "Sales Price".

```{r message=FALSE, warning=FALSE}
glimpse(combined.df)
```

## Checking for duplicates

### Property ID

In this data set, each row is a different property. The PID is the Parcel Identification Number assigned to each property within the Ames Assessors system. Checking for duplicates in relation to unique properties we see that there is none.

```{r message=FALSE, warning=FALSE}
sum(duplicated(combined.df))
    
cat("The number of duplicated rows are", nrow(combined.df) - nrow(unique(combined.df)))
```

## Missing Values

### The percentage of data missing in train

```{r message=FALSE, warning=FALSE}
sum(is.na(train)) / (nrow(train) *ncol(train))
```

### The percentage of data missing in test

```{r message=FALSE, warning=FALSE}
sum(is.na(test)) / (nrow(test) * ncol(test))
```

### The percentage of data missing in combined data set

```{r message=FALSE, warning=FALSE}
sum(is.na(combined.df)) / (nrow(combined.df) * ncol(combined.df))
```

### Summary and Distribution of the variable "SalePrice"

Sale Price is our target variable and also the dependent variable for prediction. According to the assumptions of Linear Regression, data should be normally distributed. By checking the distribution of SalePrice, we can decide if we need non-linear transformation, like log term, to make a better prediction.

```{r message=FALSE, warning=FALSE}
summary(combined.df$SalePrice)
```

```{r message=FALSE, warning=FALSE}
options(scipen=20000)
ggplot(combined.df, aes(x = SalePrice, fill = ..count..)) +
  geom_histogram(binwidth = 3000) +
  ggtitle("Distribution of Sales Price") +
  ylab("Houses") +
  xlab("Price") + 
  theme(plot.title = element_text(hjust = 0.7))
```

The target variable Sales Price is skewed to the right. To make it normally distributed we apply a log term to this variable.

### Log term of SalePrice

```{r message=FALSE, warning=FALSE}
combined.df$lSalePrice <- log(combined.df$SalePrice)

test$lSalePrice <- log(test$SalePrice)

train$lSalePrice <- log(train$SalePrice)
```

### Distribution of log SalePrice

```{r message=FALSE, warning=FALSE}
ggplot(combined.df, aes(x = lSalePrice, fill = ..count..)) +
  geom_histogram(binwidth = 0.03) +
  ggtitle("SalePrice (log) Distribution") +
  ylab("Houses") +
  xlab("Price") + 
  theme(plot.title = element_text(hjust = 0.3))
```

### Visualizing Sales Price in relation to Neighborhood.

```{r message=FALSE, warning=FALSE}
p <- ggplot(combined.df, aes(x = lSalePrice, fill = Neighborhood))
p + stat_bin()
```

### Different perspectives

The plot below shows Sales Price in relation to Neighborhood, colored by the Year it was built.

```{r message=FALSE, warning=FALSE}
q <- ggplot(combined.df, aes(x = lSalePrice, y = Neighborhood, color = YearBuilt))
q + geom_point()
```

Exploring Sales Price variable, this time in relation to Lot Area and colored by Neighborhood. We can see that the outliars are located in the same neighborhood.

```{r message=FALSE, warning=FALSE}
sales_lot <- ggplot(combined.df, aes(x = lSalePrice, y = LotArea, color = Neighborhood))
sales_lot + geom_point()
```

Log transformed x axis

```{r message=FALSE, warning=FALSE}
sales_lot2 <- ggplot(combined.df, aes(LotArea, y = 1, color = Neighborhood))
sales_lot2 + geom_jitter() + 
coord_trans(x = "log10")
```

Ground Living Area filled with Sale Condition

```{r message=FALSE, warning=FALSE}
combined.df %>%
  ggplot(aes(x = GrLivArea, fill = SaleCondition)) +
geom_bar(position = "fill") +
ylab("proportion")
```

## Residual Mean Squared Error (RMSE)

As in the MovieLens report, part important of this project is to provide the RMSE using only the training set, and experimenting with multiple parameters.

The first thing to do is to develop a function to compute the RMSE.

```{r message=FALSE, warning=FALSE}
RMSE <- function(x,y){
  a <- sqrt(sum((log(x)-log(y))^2)/length(y))
  return(a)
}
```

## Regression

A Regression predicts a numerical outcome ("dependent variable") from a set of inputs ("independent variables").

## Linear Regression

```{r message=FALSE, warning=FALSE}
model <- lm(lSalePrice ~ GrLivArea, data = train)
predict <- predict(model, test)

# RMSE
RMSE0 <- RMSE(predict, test$lSalePrice)
RMSE0 <- round(RMSE0, digits = 3)
rmse_project_results <- data_frame(Method = "Linear Regression", RMSE = RMSE0)
rmse_project_results %>% knitr::kable()
```

## Regression Trees

A decision tree is a structure in which each internal node represents a test on a feature, each leaf node represents a class label, and branches represent conjunctions of features that lead to those class labels. The paths from root to leaf represent classification rules.

```{r message=FALSE, warning=FALSE}
model <- rpart(lSalePrice ~., data = train, method = "anova")
predict <- predict(model, test)
RMSE1 <- RMSE(predict, test$lSalePrice)
RMSE1 <- round(RMSE1, digits = 3)
rmse_project_results <- bind_rows(rmse_project_results, data_frame(Method = "Regression Tree", RMSE = RMSE1))
rmse_project_results%>% knitr::kable()
```

## Random Forest

This method is also "Tree" based and it uses the qualities features of multiple Decision Trees for decision making. The term 'Random' is due to the fact that this method is a "forest" of random created [decision] 'trees'.

```{r message=FALSE, warning=FALSE}
model <- randomForest(lSalePrice ~., data = train, method = "anova",
                      ntree = 300,
                      mtry = 26,
                      replace = F,
                      nodesize = 1,
                      importance = T)
```

Visualizing the number of trees

```{r message=FALSE, warning=FALSE}
plot(model)
```

```{r message=FALSE, warning=FALSE}
predict <- predict(model, test)
RMSE2 <- RMSE(predict, test$lSalePrice)
RMSE2 <- round(RMSE2, digits = 3)
rmse_project_results <- bind_rows(rmse_project_results, data_frame(Method = "Random Forest", RMSE = RMSE2))
rmse_project_results %>% knitr::kable()
```

## Gradient Boosting Machine

Gradient Boosting Machine is a method of converting weak learners into strong learners. In boosting, each new tree is a fit on a modified version of the original data set.

```{r message=FALSE, warning=FALSE}
model <- gbm(lSalePrice ~., data = train, distribution = "laplace",
             shrinkage = 0.05,
             interaction.depth = 5,
             bag.fraction = 0.66,
             n.minobsinnode = 1,
             cv.folds = 100,
             keep.data = F,
             verbose = F,
             n.trees = 300)

predict <- predict(model, test, n.trees = 300)

RMSE3 <- RMSE(predict, test$lSalePrice)
RMSE3 <- round(RMSE3, digits = 3)
rmse_project_results <- bind_rows(rmse_project_results, data_frame(Method = "Gradient Boosting Machine", RMSE = RMSE3))
rmse_project_results %>% knitr::kable()
```

# Results and Conclusion

## RMSE Project Results

Decision Trees, Random Forests and Boosting similar machine learning methods with some overlap.In the case analyzed here, using the House Price Prediction, the best performing model was the Gradient Boosting Machine, as it combines decision trees at the beginning, instead of at the end, as Random Forest does.

```{r message=FALSE, warning=FALSE}
rmse_project_results %>% knitr::kable()
```
